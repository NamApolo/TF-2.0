{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the Notebook\n",
    "This notebook builds on a custom Linear Layer with weights being built after the layer is invoked. The main purpose is to show the use of Gradient Tape which stores the gradients and can be used manually or along with an optimizer to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Modules and Check the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models,layers\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self,units=32):\n",
    "        super(Linear,self).__init__()\n",
    "        self.units=units\n",
    "    ### The build method is to build the weights\n",
    "    def build(self,input_shape):\n",
    "        ### We will build the weight using self.add_weight using shape, initializer and setting trainable \n",
    "        ### to be true\n",
    "        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer='random_normal',trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal',trainable=True)\n",
    "    \n",
    "    ### The call method is use to do the computation required in the Layer\n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data and create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the mnist dataset from the keras datasets. Wheh load_data() is called it provides the x,y values\n",
    "### of train and test separately\n",
    "(x_train,y_train),(x_test,y_test) = keras.datasets.mnist.load_data()\n",
    "### Create the dataset using tf.data.Dataset using the from_tensor_slices method\n",
    "### This can be used when the size of the dataset is not big\n",
    "### the values will have to be reshaped into a 2D vector and normalized (by dividing by 255)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train.reshape((60000,784)).astype('float32')/255,y_train))\n",
    "### Shuffle the dataset and create batches from it\n",
    "train_ds = train_ds.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the layer, specify the parameters and train\n",
    "Using tf.GradientTape we will collect the gradients of the trainable parameters and update the weights of those parameters using these gradients using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step is 0 and Loss is 2.355255126953125\n",
      "Step is 100 and Loss is 2.2778024673461914\n",
      "Step is 200 and Loss is 2.1152334213256836\n",
      "Step is 300 and Loss is 2.0014028549194336\n",
      "Step is 400 and Loss is 1.987851619720459\n",
      "Step is 500 and Loss is 1.944572925567627\n",
      "Step is 600 and Loss is 1.76313054561615\n",
      "Step is 700 and Loss is 1.7469508647918701\n",
      "Step is 800 and Loss is 1.6767005920410156\n",
      "Step is 900 and Loss is 1.6092195510864258\n"
     ]
    }
   ],
   "source": [
    "### We will create a linear layer (created above)\n",
    "model = Linear(10)\n",
    "### Specify the losses and the optimizer. Loss Function used is Sparse Categorical Cross Entropy which is used for\n",
    "### multi class classification with int labels\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "### build and run the model\n",
    "for step, (x,y) in enumerate(train_ds):\n",
    "    ### tf.GradientTape stores the gradients of the trainable parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y,logits)\n",
    "    ### We are taking the gradients from the tf.GradientTape after optimizing for loss\n",
    "    gradients = tape.gradient(loss,model.trainable_weights)\n",
    "    ### The optimizer is used to update the weights of the trainable parameters using the gradients collected above\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n",
    "    if step%100 == 0:\n",
    "        print(\"Step is {} and Loss is {}\".format(step,loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTF]",
   "language": "python",
   "name": "conda-env-PyTF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
